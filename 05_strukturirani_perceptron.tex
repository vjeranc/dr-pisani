Perceptron \citep{rosenblatt1958perceptron} je moguće proširiti tako da se može
koristiti za rješavanje problema strukturnog predviđanja. Model je koji
dozvoljava učenje primjer po primjer \engl{online learning}, a najbrža varijanta
s najboljom generalizacijom uzima prosjek težina koje su dobivene svakim
primjerom \citep{freund1999large,gentile2002new} -- dan je pseudokod
\ref{alg:avgperceptron}. Algoritam bez usrednjavanja vraća samo $\mathbf{w}_0$ i
$b_0$.

\begin{algorithm}
\caption{Perceptron algoritam s usrednjavanjem.}\label{alg:avgperceptron}
\begin{algorithmic}[1]
\Require Skup podataka $\{x_i, y_i\}_{i=1}^N$ i broj prolaza $I$.
\State $\mathbf{w}_0 \gets \langle 0, \ldots, 0 \rangle$, $b_0 \gets 0$
\State $\mathbf{w}_a \gets \langle 0, \ldots, 0 \rangle$, $b_a \gets 0$
\State $c \gets 1$

\ForAll{$i \in \{1,2,\ldots,I\}$}

  \ForAll{$n \in \{1,2,\ldots,N\}$}
    \If{$y_n \cdot ({\mathbf{w}_0}^\top \mathbf{\Phi}(x_n) + b_0) \leq 0$}
    \State $\mathbf{w}_0 \gets \mathbf{w}_0 + y_n \mathbf{\Phi}(x_n)$, $b_0 \gets b_0 + y_n$
    \State $\mathbf{w}_a \gets \mathbf{w}_a + c y_n \mathbf{\Phi}(x_n)$, $b_a \gets b_a + c y_n$
    \EndIf
    \State $c \gets c + 1$
  \EndFor

\EndFor

\State \Return{$(\mathbf{w}_0 - \mathbf{w}_a/c, b_0 - b_a/c)$}
\end{algorithmic}
\end{algorithm}

Proširenje potrebno za rješavanje problema strukturnog predviđanja prikazano je
na pseudokodu \ref{alg:avgstructuredperceptron}.
\citet{collins2002discriminative} uvodi model strukturnog perceptrona
\engl{structured perceptron} kao zamjenu za uvjetna slučajna polja gdje je i
dalje moguće imati vrlo bogatu reprezentaciju ulaza, ali je učenje puno
jednostavnije i brže.

\begin{algorithm}
\caption{Strukturirani perceptron algoritam s usrednjavanjem.}
\label{alg:avgstructuredperceptron}
\begin{algorithmic}[1]
\Require Skup strukturiranih podataka $\{x_i, y_i\}_{i=1}^N$ i broj prolaza $I$.
\State $\mathbf{w}_0 \gets \langle 0, \ldots, 0 \rangle$
\State $\mathbf{w}_a \gets \langle 0, \ldots, 0 \rangle$
\State $c \gets 1$

\ForAll{$i \in \{1,2,\ldots,I\}$}
  \ForAll{$n \in \{1,2,\ldots,N\}$}
    \State $\hat{y}_n \gets \argmax_{y \in \mathcal{Y}} {\mathbf{w}_0}^\top \mathbf{\Phi}(x_n, y)$ \label{alg:avgstructuredperceptron:argmax}
    \If{$y_n \neq \hat{y}_n$}\label{alg:structuredperceptron:condition}
    \State $\mathbf{w}_0 \gets \mathbf{w}_0 + \mathbf{\Phi}(x_n, y_n) - \mathbf{\Phi}(x_n, \hat{y}_n)$
    \State $\mathbf{w}_a \gets \mathbf{w}_a + \mathbf{\Phi}(x_n, y_n) - \mathbf{\Phi}(x_n, \hat{y}_n)$
    \EndIf
    \State $c \gets c + 1$
  \EndFor

\EndFor

\State \Return{$\mathbf{w}_0 - \mathbf{w}_a/c$}
\end{algorithmic}
\end{algorithm}

Jedini je zahtjev da se $\argmax$-problem na liniji
\ref{alg:avgstructuredperceptron:argmax} može riješiti efikasno. U većini
slučajeva rješava se algoritmom Viterbi ili aproksimativno. Postoji pretpostavka
da je problem koji se rješava lako dekomponirati na Hammingov gubitak što je
malo bolje nego njegova aproksimacija log-gubitkom -- kao što je slučaj kod
uvjetnih slučajnih polja -- jer se optimizira pogrešna dodjela oznake izlaza (0
ili 1), a ne stupanj dodjele.

Prva pojava pretraživanja u strukturnom predviđanju pojavljuje se u
\citep{collins2004incremental}, gdje je strukturni perceptron korišten za
ovisnosno parsanje. Jedina razlika je u tome što se $\argmax$-problem rješava
koristeći algoritam \textit{beam} pretrage stoga se linija
\ref{alg:avgstructuredperceptron:argmax} zamjenjuje s $\hat{y}_n \gets
\textsc{BeamPretraga}(x_n, \mathbf{w}_0)$. Algoritam, umjesto da ima samo jedno
stablo kao hipotezu, ima ih više i time pokušava izbjeći pristranost oznakama.
Slična je ideja korištena u trenutno najtočnijem ovisnosnom parseru opisanom u
\citep{andor2016globally}. Uz \textit{beam} pretragu koriste dvoslojnu neuronsku
mrežu gdje svaki sloj ima 1024 neurona koju treniraju s globalnom
normalizacijom, ali umjesto Hammingovog gubitka koriste aproksimaciju
log-gubitkom. Kao i kod prijašnjih modela, strukturirani perceptron ima
ograničenje da funkcija gubitka mora imati dekompoziciju na izlaz $y \in
\mathcal{Y}$.
