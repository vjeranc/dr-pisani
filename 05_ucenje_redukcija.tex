Redukcije u strojnom učenju su postupak za transformiranje težih problema u
jednostavnije -- kao što je primjer redukcije problema višerazredne
klasifikacije u problem binarne klasifikacije gdje se može koristiti
jedan-protiv-svih \engl{one-against-all, abbrev.~\textsc{ova}} ili
jedan-protiv-drugog \engl{one-against-one, abbrev.~\textsc{ovo}} pristup
korištenja binarnih klasifikatora. Za \textsc{ova} pristup potrebno je $n$
klasifikatora gdje je $n$ broj razreda, a \textsc{ovo} zahtjeva $\binom{n}{2}$
klasifikatora. Prethodni ima prednost u vremenu ako je broj razreda velik, a
potonji ako je vektor gust za \textsc{ova}, a rijedak za \textsc{ovo} (onda je
moguće izbaciti značajke tj. zadržati samo one koje su vezane za dva razreda
koja uspoređujemo). Ostale prednosti ovise o samom problemu, a čak i o vrsti
binarnog klasifikatora kojeg koristimo \citep{milgram2006one}. Zanimljiv primjer
je redukcija problema višerazredne klasifikacije u stablo odluka \engl{decision
tree} gdje se svaki razred može binarno kodirati, a bitovi predstavljaju koje
ćemo dijete čvora izabrati da bi nakon $O(\log n)$ koraka stigli do potpuno
kodiranog razreda. Ovo je eksponencijalno ubrzanje s obzirom prethodne redukcije
u vremenu, a i broj klasifikatora je puno manji -- $\lceil \log n \rceil$.

Redukcije nisu toliko trivijalne kao ovi primjeri. Postoje tri potrebne
komponente: preslikavanje hipoteze, preslikavanje primjera i ograda.
Preslikavanje hipoteze opisuje postupak kako pretvoriti rješenje za jednostavan
problem u rješenje za težak. Preslikavanje primjera opisuje postupak kako
stvoriti skupove podataka za jednostavan problem koristeći skupove podataka za
težak problem. Ograda nam daje jamstvo učinkovitosti na teškom problemu ako
imamo dobru učinkovitost na jednostavnom problemu.

Postoje dvije vrste ograda koje se uzimaju u obzir: ograde na ograničenje
pogreške i ograde na ograničenje žaljenja. U slučaju redukcije koja ograničava
pogrešku, teoretsko jamstvo tvrdi da mala pogreška na jednostavnom problemu
podrazumijeva malu pogrešku na teškom problemu. U slučaju redukcije koja
ograničava žaljenje, ograda tvrdi da malo žaljenje\footnote{Žaljenje hipoteze
$h$ na problemu $\mathcal{D}$ je razlika u pogrešci kod korištenja $h$ i
korištenja najboljeg mogućeg klasifikatora. Formalno, $R(\mathcal{D}, h) =
L(\mathcal{D}, h) - \min_{h\ssymbol{1}} L(\mathcal{D}, h\ssymbol{1})$.} na
jednostavnom problemu podrayumijeva malo žaljenje na teškom. Zadovoljavajuća je
činjenica da ograde imaju kompoziciju \cite{beygelzimer2005error}. Ako možemo
reducirati problem $X$ u problem $Y$ s ogradom $f$ i problem $B$ u problem $C$ s
ogradom $g$ onda kompozicija $X \circ Y$ je redukcija s ogradom $f \circ g$.
Bitna razlika je ta što ako redukcija s ograničenjem pogreške kreira jednostavne
probleme koji su šumoviti onda bilo kakve tvrdnje o težem problemu su isprazne.

Potrebno je znati je li redukcija konzistentna \engl{consistent} da bi se
uvjerili da će redukcija raditi dobro \citep{beygelzimer2009error,
daume15reductions}. Neformalno, redukcija za problem višerazredne klasifikacije
je nekonzistentna ako skup optimalnih binarnih klasifikatora nultog žaljenja
\engl{optimal (zero-regret) binary classifer} ne može ostvariti optimalnog
višerazrednog klasifikatora u prisustvu šuma. Ispostavlja se da su gore navedene
jednostavne redukcije na stablo odluke i \textsc{ova} pristup nekonzistentne.
Navedene uvjete nekad je moguće zadovoljiti ne samo odabirom načina
zaključivanja (\textsc{ovo}, \textsc{ova} itd.) nego i načinom učenja
\citep{abe2004iterative, beygelzimer2005weighted}. Konzistentnost je osnovni
uvjet za dobru redukciju, a na žalost redukcije s ograničenjem pogreške su
uglavnom nekonzistentne. Čitatelja se upućuje na sustavni pregled zadnjih deset
godina redukcije u strojnom učenju \citep{daume15reductions}.

Redukcije su bitna sastavnica u najuspješnijim \lts{} metodama jer bez njih ne
bi imali informaciju o tome zašto su toliko uspješne nego bi pretpostavljali da
koristimo heurističke metode. Heurističke metode prisutne su u izobilju kod
primjena na probleme združenog predviđanja i bez teorije postoji samo empirijska
potvrda njihove uspješnosti (dobar primjer je razvoj metoda za ovisnosno
parsanje).
