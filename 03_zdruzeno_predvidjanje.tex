Združeno učenje i predviđanje \engl{joint learning and prediction} naziv je za
metode koje rade učenje i predviđanje jednog primjera mjereći ili minimizirajući
funkciju gubitka združeno. Naziv strukturnog učenja i predviđanja
\engl{structured learning and prediction} implicira da se funkcija gubitka
raspoređuje po komponentama strukture i da se na strukturi koja je razbijena u
svoje dijelove predviđa i uči -- rečenica u \bq{značke}{token?}, slika u piksele
i sl. \citet{daume14lts} daju jednostavnu definiciju strukturnog predviđanja kao
donošenja niza združenih odluka koristeći združenu funkciju gubitka, a
pretpostavka o tome kakav je ulaz i ima li neku strukturu ne postoji. U
\citep{daume15naacltalk} odustaju od naziva \emph{strukturno predviđanje} i
počinju probleme kategorizirati pod nazivom združeno predviđanje upravo zbog
razloga što se vrši združeni niz odluka koristeći funkciju gubitka koja će za
taj niz izračunati gubitak i ona nema nužno dekompoziciju preko niza tog odluka.
Naziv onda može uključivati združene probleme poput istovremenog označavanja
vrste riječi u rečenici i ovisnosnog parsanja rečenice, ali i svaki problem
zasebno. Kako bi nastavak bio konzistentan sa starijom literaturom koristi se
staro ime, a definicija \ref{jointlearndef} je zapravo definicija združenog
učenja.

U literaturi koja je koristila naziv za problem strukturnog predviđanja problem
nije jasno bio definiran nego je objašnjen kroz ilustrativne
primjere~\citep{mccallum2000maximum, punyakanok2001use, lafferty2001conditional,
collins2002discriminative, taskar2003maximum, mcallester2004case,
tsochantaridis2005large}. \citet{daume06thesis} definira jasnije problem, ali
kako bi pažljivo definirao problem strukturnog predviđanja postavlja dva uvjeta.

\begin{condition} \label{uvjet1}

  U problemu strukturnog predviđanja izlazni elementi $y \in \mathcal{Y}$
  dekomponiraju u vektore varijabilnih duljina preko konačnog skupa. Tj.,
  postoji konačan $M \in \field{N}$ tako da je svaki $y \in \mathcal{Y}$ moguće
  identificirati bar jednim vektorom $v_y \in M^{T_y}$, gdje je $T_y$ duljina
  vektora.

\end{condition}

Daum\'e III navodi da ovaj uvjet nije dovoljan i da uključuje probleme koje
inače ne bi smatrali strukturnim predviđanjima poput binarne klasifikacije. To
vodi do drugog uvjeta koji počinje uključivati funkciju gubitka. Želja je da
funkciju gubitka nije moguće dekomponirati preko vektorskih reprezentacija, a
kako je uvijek moguće osmisliti vektorsku reprezentaciju preko koje će funkcija
gubitka dekomponirati slijedi uvjet.

\begin{condition}

  U problemu strukturnog predviđanja funkcija gubitka nema dekompoziciju preko
  vektora $v_y$ za $y \in \mathcal{Y}$. Tj., $l(x, y, y\ssymbol{1})$ nije
  invarijantna na identične permutacije $y$ i $y\ssymbol{1}$. Ne postoji
  mapiranje $y \to v_y$ takvo da funkcija ima dekompoziciju za koju je duljina
  vektora $|v_y|$ polinomijalne funkcije koja ovisi o broju izlaznih elemenata
  $|y|$.

\end{condition}

Ovaj uvjet uspješno isključuje binarnu klasifikaciju i ostale klasifikacijske
probleme, ali isključuje i problem označavanje nizova koristeći Hammingovu
funkciju gubitka (potonja je invarijantna na permutacije). Razlika između ova
dva uvjeta je taj što u prvom značajke diktiraju strukturu, a u drugom funkcija
gubitka. Problem koji rješavamo je uvijek definiran nekom funkcijom gubitka, a
ne značajkama stoga Daum\'e III tvrdi da je problem bolje definirati preko
funkcije gubitka. Pristupi koji slijede u nastavku ovog poglavlja ne mogu
riješiti problem ako je drugi uvjet korišten za definiciju. Prihvaćena
definicija slijedi.

\begin{definition}[\citet{daume09searn}] \label{jointlearndef}

  Problem strukturnog predviđanja $\mathcal{D}$ je klasifikacijski problem
  osjetljiv na trošak \engl{cost-sensitive classification problem} gdje
  $\mathcal{Y}$ ima strukturu, a elementi $y \in \mathcal{Y}$ imaju
  dekompoziciju u vektore varijabilne duljine $(y_1, y_2, \ldots, y_T)$.
  $\mathcal{D}$ je distribucija preko ulaza $x \in \mathcal{X}$ i vektora troška
  $\mathbf{c}$ gdje je $|\mathbf{c}|$ varijabla u $2^T$.

\end{definition}

Kao primjer možemo uzeti parsiranje koristeći funkciju gubitka
\textunderscript{F}{1}. U tom slučaju je $\mathcal{D}$ distribucija preko $(x,
\mathbf{c})$ gdje je $x$ ulazni niz i za sva stabla $y$ s $|x|$ listova, $c_y$
je funkcija gubitka \textunderscript{F}{1} izlaza $y$ na točnom izlazu
$y\ssymbol{1}$. Cilj strukturnog predviđanja je pronaći funkciju $h: \mathcal{X}
\rightarrow \mathcal{Y}$ koja minimizira funkciju gubitka \ref{funcloss}. Bitno
je napomenuti kako funkcija gubitka \textunderscript{F}{1} nema dekompoziciju
preko zasebnih odluka nego se računa nakon što izgradimo cijelo stablo za
pojedinu rečenicu.

\begin{equation} \label{funcloss}
  L(\mathcal{D}, h) = \mathbb{E}_{(x, \mathbf{c}) \sim \mathcal{D}} \{c_{h(x)}\}
\end{equation}

Sve metode strukturnog predviđanja u ovom poglavlju nakon što je naučen vektor
težina $\mathbf{w}$ trebaju riješiti $\argmax$ problem \ref{argmaxproblem}.

\begin{equation}\label{argmaxproblem}
  y\ssymbol{1} = \argmax_{y \in \mathcal{Y}} \mathbf{w}^\top \phi(x, y)
\end{equation}

Ovaj problem u generalnom slučaju nije izračunljiv u dovoljno kratkom vremenu.
Ako postoji prikladan $\mathcal{Y}$ i prikladna funkcija značajki $\phi$ onda se
mogu primijeniti dinamičko programiranje \engl{dynamic programming} ili
cjelobrojno linearno programiranje \engl{integer linear programming} za
pronalaženje dobrih rješenja. Recimo, ako $\phi$ ima takvu dekompoziciju preko
vektorske reprezentacije $\mathcal{Y}$ da ni jedna značajka ne ovisi o
elementima $y$ koji su više od $k+1$ pozicija udaljeni, onda možemo iskoristiti
Viterbi algoritam za rješenje \ref{argmaxproblem}, a složenost će biti $O(M^k
T)$ -- gdje je $M$ broj mogućih oznaka, definiran u uvjetu \ref{uvjet1}, a $T$
duljina vektora na koji se dekomponira $y$ iz definicije \ref{jointlearndef}.
