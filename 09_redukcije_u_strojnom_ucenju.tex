Redukcije u strojnom učenju su postupak za transformiranje jednostavnijih
problema u složenije -- kao što je primjer redukcije problema binarne
klasifikacije u problem višerazredne klasifikacije gdje se može koristiti
jedan-protiv-svih \engla{one-against-all}{ova} ili jedan-protiv-drugog
\engla{one-against-one}{ovo} pristup korištenja binarnih klasifikatora. Za
\textsc{ova} pristup potrebno je $n$ klasifikatora gdje je $n$ broj razreda, a
\textsc{ovo} zahtijeva $\binom{n}{2}$ klasifikatora. Prethodni ima prednost u
vremenu ako je broj razreda velik, a potonji ako je vektor gust za \textsc{ova},
a rijedak za \textsc{ovo} (ako je moguće izbaciti značajke tj.~zadržati samo one
koje su vezane za dva razreda koja uspoređujemo). Ostale prednosti ovise o samom
problemu, a čak i o vrsti binarnog klasifikatora kojeg koristimo
\citep{milgram2006one}. Zanimljiv je primjer redukcija problema višerazredne
klasifikacije stablom odluka \engl{decision tree} gdje se svaki razred može
binarno kodirati, a bitovi predstavljaju koje ćemo dijete čvora izabrati da bi
nakon $O(\log n)$ koraka stigli do potpuno kodiranog razreda
\citep{beygelzimer2009error, daume2016one}. Ovo je eksponencijalno ubrzanje s
obzirom na prethodne redukcije u vremenu, a i broj klasifikatora je puno manji
-- $\lceil \log n \rceil$.

Pitanje je jesu li redukcije učinkovit pristup za rješavanje složenih problema
strojnog učenja? Odgovor nije očit jer je teško opisati reprezentaciju na
trivijalan matematički način. Moguće je da redukcija stvori jednostavne probleme
koje nije lako riješiti. Trivijalan je primjer trorazredne klasifikacije s
jednom značajkom i linearnim binarnim klasifikatorom. Kod \textsc{ova} pristupa
nije moguće odvojiti razrede s dovoljno malom pogreškom, a kod \textsc{ovo} je
to moguće.

Redukcije nisu toliko trivijalne kao ovi primjeri. Postoje tri potrebne
komponente:
\begin{inlinelist}
  \item preslikavanje hipoteze,
  \item preslikavanje primjera i
  \item analiza ograde.
\end{inlinelist}
Preslikavanje hipoteze opisuje postupak kako pretvoriti rješenje za jednostavan
problem u rješenje za težak. Preslikavanje primjera opisuje postupak kako
stvoriti skupove podataka za jednostavan problem koristeći skupove podataka za
težak problem. Ograda nam daje jamstvo učinkovitosti na teškom problemu ako
imamo dobru učinkovitost na jednostavnom problemu.

Postoje dvije vrste ograda koje se uzimaju u obzir: ograde na ograničenje
pogreške \engl{error bounds} i ograde na ograničenje žaljenja \engl{regret
bounds}. U slučaju redukcije koja ograničava pogrešku, teoretsko jamstvo tvrdi
da mala pogreška na jednostavnom problemu podrazumijeva malu pogrešku na teškom
problemu. U slučaju redukcije koja ograničava žaljenje, ograda tvrdi da malo
žaljenje na jednostavnom problemu podrazumijeva malo žaljenje na
teškom.\footnote{Žaljenje hipoteze $h$ na problemu $\mathcal{D}$ je razlika u
pogrešci kod korištenja $h$ i korištenja najboljeg mogućeg klasifikatora.
Formalno, $R(\mathcal{D}, h) = L(\mathcal{D}, h) - \min_{h\ssymbol{1}}
L(\mathcal{D}, h\ssymbol{1})$.} Zadovoljavajuća je činjenica da ograde imaju
kompoziciju \citep{beygelzimer2005error}. Ako možemo reducirati problem $X$ u
problem $Y$ s ogradom $f$ i problem $Y$ u problem $Z$ s ogradom $g$ onda
kompozicija $X \circ Y$ je redukcija s ogradom $f \circ g$. Ovo svojstvo
pojednostavljuje matematičku analizu ograda redukcije. Bitna razlika između
navedenih ograda je ta što ako redukcija s ograničenjem pogreške kreira
jednostavne probleme koji su šumoviti onda su bilo kakve tvrdnje o težem
problemu isprazne.

Potrebno je znati je li redukcija konzistentna \engl{consistent} da bi se
uvjerili da će redukcija raditi dobro \citep{beygelzimer2009error,
daume15reductions}. Neformalno, redukcija koja transformira bilo koje optimalno
rješenje za osnovni jednostavni problem u optimalno rješenje za složeniji
problem je \textit{konzistentna}. Ispostavlja se da su gore navedene jednostavne
redukcije stablom odluke i \textsc{ova} pristup nekonzistentne. Navedene uvjete
nekad je moguće zadovoljiti ne samo odabirom načina zaključivanja (\textsc{ovo},
\textsc{ova} i dr.) nego i načinom učenja \citep{abe2004iterative,
beygelzimer2005weighted}. Konzistentnost je osnovni uvjet za dobru redukciju, a
na žalost redukcije s ograničenjem pogreške su uglavnom nekonzistentne.
Čitatelja se upućuje na sustavni pregled zadnjih deset godina redukcije u
strojnom učenju \citep{daume15reductions}.

Redukcije su bitna sastavnica u najuspješnijim \lts{} metodama jer bez njih ne
bi imali informaciju o tome zašto su toliko uspješne nego bi pretpostavljali da
koristimo heurističke metode. Heurističke metode prisutne su u izobilju kod
primjena na probleme združenog predviđanja i bez teorije postoji samo empirijska
potvrda njihove uspješnosti.\footnote{Dobar primjer je razvoj metoda za
ovisnosno parsiranje koje koriste \textit{beam} pretragu i time eksperimentalno
poboljšavaju performanse \citep{zhang2011transition, bohnet2012transition}.}
