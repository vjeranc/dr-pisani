@article{daume09searn,
  author =       {Hal {Daum\'e III} and John Langford and Daniel Marcu},
  title =        {Search-based Structured Prediction},
  year =         {2009},
  booktitle =    {Machine Learning Journal (MLJ)},
  abstract =     {
    We present Searn, an algorithm for integrating search and
    learning to solve complex structured prediction problems such
    as those that occur in natural language, speech, computational
    biology, and vision.  Searn is a meta-algorithm that transforms
    these complex problems into simple classification problems to which
    any binary classifier may be applied.  Unlike current algorithms for
    structured learning that require decomposition of both the loss
    function and the feature functions over the predicted structure,
    Searn is able to learn prediction functions for any loss
    function and any class of features.  Moreover, Searn comes
    with a strong, natural theoretical guarantee: good performance on the
    derived classification problems implies good performance on the
    structured prediction problem.
  },

}

@PhdThesis{daume06thesis,
  author =       {Hal {Daum\'e III}},
  title =        {Practical Structured Learning Techniques for Natural Language Processing},
  school =       {University of Southern California},
  year =         {2006},
  address =      {Los Angeles, CA},
  month =        {Kolovoz},
  keywords = {sp nlp ml},
  tagline = {<i>Committee:</i> <a href="http://www.isi.edu/~marcu/"><font color="8866DD">D. Marcu</font></a>, <a href="http://www.isi.edu/~knight/"><font color="8866DD">K. Knight</font></a>, <a href="http://www.isi.edu/~hovy/"><font color="8866DD">E. Hovy</font></a>, <a href="http://www-clmc.usc.edu/~sschaal/"><font color="8866DD">S. Schaal</font></a>, <a href="http://www-rcf.usc.edu/~gareth/"><font color="8866DD">G. James</font></a>, <a href="http://www.cs.umass.edu/~mccallum/"><font color="8866DD">A. McCallum</font></a>.<br> This thesis describes an algorithm for solving many of the complex prediction problems encountered in NLP applications. The algorithm comes with strong theoretical guarentees, is empirically effective in applications such as IE and summarization, is efficient and is easy to implement.},

}

@unpublished{daume06searn-practice,
  author =       {Hal {Daum\'e III} and John Langford and Daniel Marcu},
  title =        {Searn in Practice},
  year =         {2006},
  abstract = {
    We recently introduced an algorithm, Searn, for solving hard
    structured prediction problems.  This algorithm
    enjoys many nice properties: efficiency, wide applicability,
    theoretical justification and simplicity.  However, under a desire to
    fit a lot of information into the original paper,
    it may not be so clear how simple the technique is.  This report is
    designed to showcase how Searn can be applied to a wide variety of
    techniques and what really goes on behind the scenes.  We will
    make use of three example problems, ranging from simple to complex.
    These are: (1) sequence labeling, (2) parsing and (3) machine
    translation.  (These were chosen to be as widely understandable,
    especially in the NLP community, as possible.)  In the end, we will
    come back to discuss Searn for general problems.
  },
  keywords = {nlp ml sp},
  tagline = {We recently introduced an algorithm, Searn, for solving hard structured prediction problems. This report is designed to showcase how Searn can be applied to a wide variety of techniques and what really goes on behind the scenes. We show how to apply Searn to three common NLP problems: (1) sequence labeling, (2) parsing and (3) machine translation.},

}

@inproceedings{daume15reductions,
    title     = {Learning Reductions that Really Work},
    author = {Alina Beygelzimer and Hal {Daum\'e III} and John Langford and Paul
                    Mineiro},
    booktitle = {IEEE Proceedings},
    year      = {2015},

}

@inproceedings{daume15lols,
    title     = {Learning to search better than your teacher},
    author = {Kai-Wei Chang and Akshay Krishnamurthy and Alekh Agarwal and Hal
                    {Daum\'e III} and John Langford},
    booktitle = {Proceedings of the International Conference on Machine Learning
                    (ICML)},
    year      = {2015},

}

@inproceedings{daume15rewrite,
    title     = {Syntax-based Rewriting for Simultaneous Machine Translation},
    author    = {He He and Alvin Grissom II and Jordan Boyd Graber and Hal {Daum\'e III}},
    booktitle = {Proceedings of the Conference on Empirical Methods in Natural
                    Language Processing (EMNLP)},
    year      = {2015},
}

@inproceedings{daume14lts,
    title     = {Efficient programmable learning to search},
    author = {Kai-Wei Chang and Hal {Daum\'e III} and John Langford and St\'ephane
                    Ross},
    booktitle = {arXiv},
    year      = {2014},

}

@article{lafferty2001conditional,
  title={Conditional random fields: Probabilistic models for segmenting and labeling sequence data},
  author={Lafferty, John and McCallum, Andrew and Pereira, Fernando CN},
  year={2001}
}

@phdthesis{leon1991approche,
  title={Une Approche théorique de l’Apprentissage Connexioniste;
Applications à la reconnaissance de la Parole.},
  author={Léon Bottou},
  year={1991}
}

@inproceedings{mccallum2000maximum,
  title={Maximum Entropy Markov Models for Information Extraction and Segmentation.},
  author={McCallum, Andrew and Freitag, Dayne and Pereira, Fernando CN},
  booktitle={ICML},
  volume={17},
  pages={591--598},
  year={2000}
}

@article{punyakanok2001use,
  title={The use of classifiers in sequential inference},
  author={Punyakanok, Vasin and Roth, Dan},
  journal={arXiv preprint cs/0111003},
  year={2001}
}

@inproceedings{collins2002discriminative,
  title={Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms},
  author={Collins, Michael},
  booktitle={Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10},
  pages={1--8},
  year={2002},
  organization={Association for Computational Linguistics}
}

@article{taskar2003maximum,
  title={Maximum-margin markov networks},
  author={Taskar, B and Guestrin, C and Koller, D},
  journal={Advances in neural information processing systems (NIPS)},
  year={2003}
}

@inproceedings{mcallester2004case,
  title={Case-factor diagrams for structured probabilistic modeling},
  author={McAllester, David and Collins, Michael and Pereira, Fernando},
  booktitle={Proceedings of the 20th conference on Uncertainty in artificial intelligence},
  pages={382--391},
  year={2004},
  organization={AUAI Press}
}

@inproceedings{tsochantaridis2005large,
  title={Large margin methods for structured and interdependent output variables},
  author={Tsochantaridis, Ioannis and Joachims, Thorsten and Hofmann, Thomas and Altun, Yasemin},
  booktitle={Journal of Machine Learning Research},
  pages={1453--1484},
  year={2005}
}

@misc{daume15naacltalk,
title= {Hands-on Learning to Search for Structured Prediction},
author = {Daumé III, Hal and Lanford, John, and Chang, Kai-Wei and He, He and Rao, Sudha},
year = {2015},
note= {North American Chapter of the Association for Computational Linguistics},
URL= {http://hunch.net/~l2s/},
}
