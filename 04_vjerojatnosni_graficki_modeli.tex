Najpoznatiji vjerojatnosni grafički modeli korišteni za probleme strukturnog
predviđanja su skriveni Markovljevi modeli \engla{hidden Markov models}{hmm},
Markovljev model maksimalne entropije \engla{maximum entropy Markov model}{memm}
i uvjetna slučajna polja \engla{conditional random fields}{crf}. Svaki ima
svoje prednosti. Učenje skrivenog Markovljevog modela moguće je izvesti jako
brzo ako je na raspolaganju velika količina podataka. Prijelazne vjerojatnosti
$p(x_i | y_i)$ i $p(y_i | y_{i-1})$ moguće je naučiti vrlo brzo
prebrojavanjem supojavljivanja. Kod uvjetnih slučajnih polja lakše je opisati
ulaz značajkama što omogućava bolju generalizaciju iz manje količine podataka,
ali proces učenja je dugotrajniji. Markovljev model maksimalne entropije je po
učinkovitosti između dva modela, omogućava brzo učenje i dobru generalizaciju,
ali je primijećeno da način učenja dovodi do problema pristranosti oznakama
\engl{label bias} \citep{lafferty2001conditional}, a problem je detaljnije
obrađen u potpoglavlju \ref{ch:labelbias}.

\begin{figure}[H]
\tikzstyle{lightedge}=[<-,dotted]
\tikzstyle{mainstate}=[state,thick]
\tikzstyle{mainedge}=[<-,thick]
\tikzstyle{undirect}=[shape=rectangle,draw=black,fill=black]

\begin{subfigure}[b]{1\textwidth}
\begin{center}
\begin{tikzpicture}[scale=1.15, every node/.style={transform shape}]
\tikzstyle{state}=[shape=circle,draw=black,fill=black!8,minimum size=32pt]
\tikzstyle{observation}=[shape=circle,draw=black,fill=black!8,minimum size=32pt]
% states
\node[state] (s1) at (0,2) {$y_{i-1}$};
\node[state] (s2) at (2,2) {$y_{i\,\,}$}
    edge [mainedge,bend right=45] node[auto,swap] {$p(y_{i}|y_{i-1})$} (s1);
\node[state] (s3) at (4,2) {$y_{i+1}$}
    edge [mainedge,bend right=45] node[auto,swap] {$p(y_{i+1}|y_{i})$} (s2);
% observations
\node[observation] (x1) at (0,0) {$x_{i-1}$}
    edge [mainedge] node[left] {$p(x_{i}|y_{i})$} (s1);
\node[observation] (x2) at (2,0) {$x_{i\,\,}$}
    edge [mainedge] (s2);
\node[observation] (x3) at (4,0) {$x_{i+1}$}
    edge [mainedge] (s3);
\end{tikzpicture}
\caption{Skriveni Markovljev model}
\label{modeli1:hmm}
\end{center}
\end{subfigure}

\begin{subfigure}[b]{0.5\textwidth}
\begin{center}
\begin{tikzpicture}[scale=1.15, every node/.style={transform shape}]
\tikzstyle{state}=[shape=circle,draw=black,fill=black!8,minimum size=32pt]
\tikzstyle{observation}=[shape=circle,draw=black,fill=white!20,minimum size=32pt]
% states
\node[state] (s1) at (0,2) {$y_{i-1}$};
\node[state] (s2) at (2,2) {$y_{i}$}
    edge [mainedge,bend right=45] node[auto,swap] {$p(y_{i}|y_{i-1})$} (s1);
\node[state] (s3) at (4,2) {$y_{i+1}$}
    edge [mainedge,bend right=45] node[auto,swap] {$p(y_{i+1}|y_{i})$} (s2);
% observations
\node[observation] (x1) at (0,0) {$x_{i-1}$}
    edge [->,thick] node[left] {$p(y_{i}|x_{i})$} (s1);
\node[observation] (x2) at (2,0) {$x_{i}$}
    edge [->,thick] (s2);
\node[observation] (x3) at (4,0) {$x_{i+1}$}
    edge [->,thick] (s3);
\end{tikzpicture}
\caption{Markovljev model maksimalne entropije}
\end{center}
\end{subfigure}
\begin{subfigure}[b]{0.5\textwidth}
\begin{center}
\begin{tikzpicture}[scale=1.15, every node/.style={transform shape}]
\tikzstyle{state}=[shape=circle,draw=black,fill=black!8,minimum size=32pt]
\tikzstyle{observation}=[shape=circle,draw=black,fill=white!20,minimum size=32pt]
% states
\node[state] (s1) at (0,2) {$y_{i-1}$};
\node[state] (s2) at (2,2) {$y_{i}$}
    edge [thick] node[undirect] {} (s1);
\node[state] (s3) at (4,2) {$y_{i+1}$}
    edge [thick] node[undirect] {} (s2);
% observations
\node[observation] (x1) at (0,0) {$x_{i-1}$}
    edge [thick] node[undirect] {} (s1);
\node[observation] (x2) at (2,0) {$x_{i}$}
    edge [thick] node[undirect] {} (s2);
\node[observation] (x3) at (4,0) {$x_{i+1}$}
    edge [thick] node[undirect] {} (s3);
\end{tikzpicture}
\caption{Uvjetna slučajna polja}
\end{center}
\end{subfigure}
\caption[Prikaz predložaka grafičkih modela.]{Prikaz predložaka grafičkih
modela. Sivi čvorovi predstavljaju varijable koje model može generirati, a
bijeli koje model može samo opaziti.}
\label{fig:grafickimodeli}
\end{figure}


\subsection{Skriveni Markovljev model}

Koristi se najčešće kod problema gdje prijelazne vjerojatnosti modeliraju niz --
kao što je prikazano na slici \ref{fig:grafickimodeli}. To čini model prikladnim
za problem označavanja vrste riječi \citep{halacsy2007hunpos} i prepoznavanja
imenovanih entiteta \citep{zhou2002named}. Kako u većini slučajeva skup za
učenje nije dovoljno velik, uobičajeno je dodati neke vanjske informacije --
poput toga da se imenovani entitet koji nije bio u skupu za učenje pojavljuje u
nekom rječniku koji je na raspolaganju ili da riječi s određenim prefiksom su
češće imenice nego glagoli -- onda se algoritam učenja brojanjem supojavljivanja
zamjenjuje Baum-Welch algoritmom koji ima složenost $O(T M^k)$ -- gdje je $T$
duljina niza, $M$ broj oznaka koje je moguće predvidjeti za svaki izlaz i izlaz ne
ovisi o elementima u $y$ koji su $k$ ili više pozicija udaljeni, ali pokazalo se
da algoritam jako teško dolazi do dobrog lokalnog optimuma te se za navedene
probleme ipak koriste drugi modeli \citep{johnson2007doesn}.

\subsection{Markovljev model maksimalne entropije}

Markovljev model maksimalne entropije \engl{maximum entropy Markov model}
izravno su primijenili \citet*{mccallum2000maximum} koristeći model maksimalne
entropije (logističke regresije) na probleme označavanja nizova. Model je vrlo
sličan skrivenom Markovljevom modelu \engl{hidden Markov model}, ali je
fleksibilniji jer omogućava veću slobodu u odabiru značajki opaženih varijabli.
Modelira se uvjetna vjerojatnost $i$-te oznake izlaza $y_i$ za dane $x$ i
prijašnje oznake $y_{i-1}$ prikazana u jednadžbi \ref{eq:maxent1}.

\begin{equation}\label{eq:maxent1}
\begin{aligned}
  p(y_i | x, y_{i-1}; \mathbf{w}) = \frac{1}{Z(x, y_{i-1}; \mathbf{w})} \exp \big[ \mathbf{w}^\top \mathbf{\Phi}(x, y_i, y_{i-1})\big] \\
  Z(x, y_{i-1}; \mathbf{w}) = \sum_{y' \in \mathcal{Y}} \exp \big[ \mathbf{w}^\top \mathbf{\Phi}(x, y', y_{i-1})\big]
\end{aligned}
\end{equation}

\noindent
Model se uči koristeći prave izlazne nizove kao skup podataka za učenje i
koriste se pravi $y_{i-1}$ za generiranje primjera za učenje. Navedeni postupak
proizvede primjere za učenje višerazredne klasifikacije. Broj primjera jednak je
broju svih pojavljivanja oznaka u cijelom skupu za učenje (npr.~za problem
označavanja niza to bi bio zbroj duljina svih nizova). Taj generirani skup
koristi se za učenje vektora $\mathbf{w}$ postupkom koji je identičan učenju
modela maksimalne entropije -- za učenje se može koristiti neka varijanta
gradijentnog spusta.

Kod zaključivanja koristi se algoritam Viterbi za rješavanje $\argmax$-problema
(\ref{eq:argmaxproblem}). Zbog ove kombinacije algoritama učenja i zaključivanja
dolazi do problema pristranosti oznakama (potpoglavlje \ref{ch:labelbias}).

\cite{cohen05ijcai} pokušali su smanjiti utjecaj pristranosti oznakama i
uspješnost je usporediva s uvjetnim slučajnim poljima, ali sve prednosti vezane
uz vremensku učinkovitost učenja iščezavaju. Unatoč ovim promjenama model se
može primjenjivati samo na probleme označavanja nizova.


\subsection{Uvjetna slučajna polja}

\citet*{lafferty2001conditional} uvode uvjetna slučajna polja kao rješenje
problema pristranosti oznakama i eksperimentalno potvrđuju njegovo nepostojanje.
Problem je poznat i ranije kod primjene neuronskih Markovljevih modela na slične
probleme \citep{leon1991approche}. Uvjetna slučajna polja nemaju određenu
funkciju gubitka, nego koriste log-gubitak kao aproksimaciju Hammingovog gubitka
preko cijelog strukturiranog izlaza. Zbog toga ih je moguće primijeniti samo na
probleme čija se funkcija gubitka može dekomponirati po elementima. Modelira se
uvjetna vjerojatnost prikazana u jednadžbi \ref{eq:crf}.

\begin{equation}\label{eq:crf}
\begin{aligned}
  p(y | x; \mathbf{w}) = \frac{1}{Z(x; \mathbf{w})} \exp \big[ \mathbf{w}^\top \mathbf{\Phi}(x, y)\big] \\
  Z(x; \mathbf{w}) = \sum_{y' \in \mathcal{Y}} \exp \big[ \mathbf{w}^\top \mathbf{\Phi}(x, y')\big]
\end{aligned}
\end{equation}

\noindent
Particijska funkcija $Z(x; \mathbf{w})$ u zbroju koristi sve izlaze $y'$ za koje
model daje kriva predviđanja. Ako se koristi iterativni algoritam učenja, onda
se za pravilnu procjenu parametara taj zbroj računa svaki put prije korigiranja
težinskog vektora $\mathbf{w}$. Taj je zbroj u generalnom slučaju prevelik za
efikasno računanje, ali ako se pretpostavi struktura linearnog lanca, onda ga je
moguće izračunati koristeći dinamičko programiranje i svaka prilagodba na novi
problem koji je predstavljen netrivijalnim linearnim lancem zahtijeva novu
implementaciju algoritama učenja i zaključivanja \citep{lafferty2001conditional,
sha2003shallow}.

Algoritam za računanje zbroja $Z(x; \mathbf{w})$ ima složenost $O(T M ^ k)$ i
gotovo je identičan algoritmu \textit{forward-backward}
\citep{baum1966statistical}.

Kao što je slučaj kod modela maksimalne entropije težine $\mathbf{w}$ moguće je
regularizirati i koristi se log-posteriorna distribucija preko težina opisana u
jednadžbi \ref{eq:crflog}.

\begin{equation}\label{eq:crflog}
  \log p(\mathbf{w} | \mathcal{D}; \sigma^2) = -\frac{1}{\sigma^2} {\lVert\mathbf{w}\lVert}^2 + \sum_{(x_n, y_n) \in \mathcal{D}} \bigg[ \mathbf{w}^\top \mathbf{\Phi}(x_n, y_n) - \log Z(x_n; \mathbf{w}) \bigg]
\end{equation}

\noindent
Pronalaženje optimalnog vektora težina $\mathbf{w}$ rješava se na razne načine
\citep{lafferty2001conditional, sha2003shallow, sokolovska2010efficient} i
ovisno o problemu neke je moguće ili nemoguće primijeniti, a za više detalja
čitatelja se upućuje na \citep{wallach2004conditional, sutton2006introduction}.
Zaključak je da se uvjetna slučajna polja mogu primjenjivati na probleme gdje
se efikasno može riješiti $\argmax$-problem (\ref{eq:argmaxproblem}) i
izračunati particijska funkcija $Z(x; \mathbf{w})$. Funkcija gubitka mora imati
takvu dekompoziciju preko izlaza $y \in \mathcal{Y}$ da je moguće log-gubitkom
aproksimirati Hammingov gubitak. Unatoč navedenim uvjetima model je našao
primjene u označavanju vrste riječi \citep{lafferty2001conditional},
prepoznavanju imenovanih entiteta \citep{mccallum2003early,
settles2004biomedical}, sažimanju dokumenata \engl{document summarization}
\citep{shen2007document}, sentiment analizi \citep{mcdonald2007structured} i dr.
U svakoj primjeni funkcija gubitka je imala dekompoziciju preko niza odluka i
algoritmi su se morali prilagoditi za pravilno zaključivanje ili se odbacivanjem
točnog zaključivanja koristilo aproksimativno.
