Redukcije u strojnom učenju su postupak za transformiranje težih problema u
jednostavnije -- kao što je primjer redukcije problema višerazredne
klasifikacije u problem binarne klasifikacije gdje se može koristiti
jedan-protiv-svih \engla{one-against-all}{ova} ili jedan-protiv-drugog
\engla{one-against-one}{ovo} pristup korištenja binarnih klasifikatora. Za
\textsc{ova} pristup potrebno je $n$ klasifikatora gdje je $n$ broj razreda, a
\textsc{ovo} zahtjeva $\binom{n}{2}$ klasifikatora. Prethodni ima prednost u
vremenu ako je broj razreda velik, a potonji ako je vektor gust za \textsc{ova},
a rijedak za \textsc{ovo} (ako je moguće izbaciti značajke tj.~zadržati samo one
koje su vezane za dva razreda koja uspoređujemo). Ostale prednosti ovise o samom
problemu, a čak i o vrsti binarnog klasifikatora kojeg koristimo
\citep{milgram2006one}. Zanimljiv je primjer redukcija problema višerazredne
klasifikacije u stablo odluka \engl{decision tree} gdje se svaki razred može
binarno kodirati, a bitovi predstavljaju koje ćemo dijete čvora izabrati da bi
nakon $O(\log n)$ koraka stigli do potpuno kodiranog razreda. Ovo je
eksponencijalno ubrzanje s obzirom prethodne redukcije u vremenu, a i broj
klasifikatora je puno manji -- $\lceil \log n \rceil$.

Pitanje je jesu li redukcije učinkovit pristup za rješavanje složenih problema
strojnog učenja? Odgovor nije očit jer je teško opisati reprezentaciju na
trivijalan matematički način. Moguće je da redukcija stvori jednostavne probleme
koje nije lako riješiti. Trivijalan je primjer trorazredne klasifikacije s
jednom značajkom i linearnim binarnim klasifikatorom. U slučaju \textsc{ova}
pristupa nećemo moći odvojiti razrede s dovoljno malom pogreškom, a \textsc{ovo}
neće imati problema.

Redukcije nisu toliko trivijalne kao ovi primjeri. Postoje tri potrebne
komponente: preslikavanje hipoteze, preslikavanje primjera i analiza ograde.
Preslikavanje hipoteze opisuje postupak kako pretvoriti rješenje za jednostavan
problem u rješenje za težak. Preslikavanje primjera opisuje postupak kako
stvoriti skupove podataka za jednostavan problem koristeći skupove podataka za
težak problem. Ograda nam daje jamstvo učinkovitosti na teškom problemu ako
imamo dobru učinkovitost na jednostavnom problemu.

Postoje dvije vrste ograda koje se uzimaju u obzir: ograde na ograničenje
pogreške i ograde na ograničenje žaljenja. U slučaju redukcije koja ograničava
pogrešku, teoretsko jamstvo tvrdi da mala pogreška na jednostavnom problemu
podrazumijeva malu pogrešku na teškom problemu. U slučaju redukcije koja
ograničava žaljenje, ograda tvrdi da malo žaljenje\footnote{Žaljenje hipoteze
$h$ na problemu $\mathcal{D}$ je razlika u pogrešci kod korištenja $h$ i
korištenja najboljeg mogućeg klasifikatora. Formalno, $R(\mathcal{D}, h) =
L(\mathcal{D}, h) - \min_{h\ssymbol{1}} L(\mathcal{D}, h\ssymbol{1})$.} na
jednostavnom problemu podrazumijeva malo žaljenje na teškom. Zadovoljavajuća je
činjenica da ograde imaju kompoziciju \citep{beygelzimer2005error}. Ako možemo
reducirati problem $X$ u problem $Y$ s ogradom $f$ i problem $B$ u problem $C$ s
ogradom $g$ onda kompozicija $X \circ Y$ je redukcija s ogradom $f \circ g$.
Bitna razlika između navedenih ograda je ta što ako redukcija s ograničenjem
pogreške kreira jednostavne probleme koji su šumoviti onda su bilo kakve tvrdnje
o težem problemu isprazne.

Potrebno je znati je li redukcija konzistentna \engl{consistent} da bi se
uvjerili da će redukcija raditi dobro \citep{beygelzimer2009error,
daume15reductions}. Neformalno, redukcija koja transformira bilo koje optimalno
rješenje za osnovni jednostavni problem u optimalno rješenje za složeniji
problem je \textit{konzistentna}. Ispostavlja se da su gore navedene jednostavne
redukcije na stablo odluke i \textsc{ova} pristup nekonzistentne. Navedene
uvjete nekad je moguće zadovoljiti ne samo odabirom načina zaključivanja
(\textsc{ovo}, \textsc{ova} itd.) nego i načinom učenja \citep{abe2004iterative,
beygelzimer2005weighted}. Konzistentnost je osnovni uvjet za dobru redukciju, a
na žalost redukcije s ograničenjem pogreške su uglavnom nekonzistentne.
Čitatelja se upućuje na sustavni pregled zadnjih deset godina redukcije u
strojnom učenju \citep{daume15reductions}.

Redukcije su bitna sastavnica u najuspješnijim \lts{} metodama jer bez njih ne
bi imali informaciju o tome zašto su toliko uspješne nego bi pretpostavljali da
koristimo heurističke metode. Heurističke metode prisutne su u izobilju kod
primjena na probleme združenog predviđanja i bez teorije postoji samo empirijska
potvrda njihove uspješnosti.\footnote{Dobar primjer je razvoj metoda za
ovisnosno parsanje koje koriste \textit{beam} pretragu i time eksperimentalno
poboljšavaju performanse \citep{zhang2011transition, bohnet2012transition}.}
